{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootcamp_Projeto_Final(ML)",
      "provenance": [],
      "collapsed_sections": [
        "Idg0MpAL4bMa",
        "kRxgMrAgMYeh",
        "uGAXRzIDOkqI"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJX8o6VYZhN"
      },
      "source": [
        "#Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53PN4saQa67R"
      },
      "source": [
        "## Apresentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PlpNGPYcmc"
      },
      "source": [
        "Este é o segundo notebook do [Projeto Final do Bootcamp da Alura](https://github.com/renanmath/Bootcamp_Projeto_Final). [Clique aqui](https://github.com/renanmath/Bootcamp_Projeto_Final/tree/main/notebooks) para acessar os demais notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXrIIOO2Zod8"
      },
      "source": [
        "No primeiro notebook, fizemos o tratamento dos dados e uma análise exploratória. Ao final dele, geramos um arquivo csv com os dados já preparados e subimos uma cópia no repositório do projeto no Github, de onde iremos acessá-lo remotamente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps8z4u5daNVd"
      },
      "source": [
        "Este notebook está organizado da seguinte maneira:\n",
        "- Introdução\n",
        "- Importação das bibliotecas e criação das funções\n",
        "- Importação dos dados\n",
        "- Criação das variáveis\n",
        "- Criação do modelo baseline (modelo simples)\n",
        "- Criação dos modelos intermediários\n",
        "- Criação do modelo final\n",
        "- Considerações finais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZjtSeja45M"
      },
      "source": [
        "## Entendendo o problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMuORIDla_wl"
      },
      "source": [
        "Como vimos no primero notebook, esse projeto é inspirado em um desafio proposto pelo Hospital Sírio Libanês no [Kaggle](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19). Os dados apresentados contém informações sobre alguns pacientes do hospital. Essas informações são:\n",
        "- Dados demográficos\n",
        "- Resultados dos testes sanguíneos\n",
        "- Medições de sinais vitais\n",
        "- Janela de tempo de internação\n",
        "- Se o paciente foi ou não para a UTI\n",
        "\n",
        "A partir desses dados, o desafio propõe criar modelos preditivos que possam responder as seguintes perguntas:\n",
        "\n",
        "\n",
        "1.   O paciente irá para a UTI nas próximas horas?\n",
        "2.   O paciente não irá para a UTI nas próximas horas?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrmIhFacoZt"
      },
      "source": [
        "Apesar de parecidas, essas duas perguntas são distintas. Um modelo que possa responder satisifatoriamente uma delas, não necessariamente será bom para responder a outra. Para entender isso melhor, precisamos discutir brevemente algumas métricas que serão usadas em nossos estudos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85TtCngtdsPq"
      },
      "source": [
        "A *acurácia* ('*score*', em inglês) é provavelmente a métrica mais lembrada, embora não a mais importante. Ela é definida  como o total de acertos / total de amostras. Ele, entretanto, não consegue capturar sutilezas que outras métricas levam em contam, e por isso é um tanto imprecisa.\n",
        "\n",
        "Uma boa maneira de começar a melhorar a análise da performace de um modelo é verificar sua *matriz de confusão* ('*confusion matrix*'). Nas linhas dessa matriz temos os resultados reais e nas colunas temos os resultados previstos pelo modelo, de acordo com cada classe.\n",
        "\n",
        "Em problemas de classificação binária, será uma matrix 2x2.\n",
        "\n",
        "Nas células dessa matriz, temos 4 informações importantes:\n",
        "\n",
        "- VP = número de verdadeiros positivos (modelo previu que era positivo e de fato era positivo)\n",
        "- VN = número de verdadeiros negativos (modelo previu que era negativo e de fato era negativo)\n",
        "- FP = número de falsos positivos (modelo previu que era positivo, mas na verdade era negativo)\n",
        "- FN = número de falsos negativos (modelo previu que era negativo, mas na verdade era positivo)\n",
        "\n",
        "Temos métricas que levam esses números em conta:\n",
        "\n",
        "- precisão de positos = VP/(VP+FP)\n",
        "- precisão de negativos = VN/(VN+FN)\n",
        "- recall (ou revocação) de positivos = VP/(VP+FN)\n",
        "- recall (ou revocação) de negativos = VN/(VN+FP)\n",
        "- F1 = média harmônica entre presicão e revocação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur9t1ZM_epp4"
      },
      "source": [
        "Com essas terminologias, podemos dizer de forma mais precisa quando um modelo é bom para resolver o problema 1 e quando é bom para resolver o problema 2. No primeiro caso, queremos evitar falsos positivos; no segundo, queremos evitar falsos negativos. Assim:\n",
        "\n",
        "- Um modelo que é bom em prever se um paciente irá para a UTI deve ter valores altos de precisão de positivos e revocação de negativos, pois ele prevê poucos falsos positivos. \n",
        "- Um modelo que é bom em prever se um paciente não irão para a UTI deve ter valores altos de precisão de negativos e revocação de positivos, pois ele prevê poucos falsos negativos. \n",
        "\n",
        "Como veremos, balancear essas métricas podem ser muito complicado. No geral, foi mais fácil responder o problema 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ihZYeGkgT9j"
      },
      "source": [
        "Como visto também no primeiro notebook, o banco de dados inicial continha uma coluna WINDOW, que mostra a janela de tempo (desde a admissão) de referência para os dados de cada linha. Os possíveis valores eram:\n",
        "- 0-2 -----> de zero a duas horas após a adimissão\n",
        "- 2-4 -----> de duas a quatro horas após a adimissão\n",
        "- 4-6 -----> de quatro a seis horas após a adimissão\n",
        "- 6-12 ----> de seis a doze horas após a adimissão\n",
        "- ABOVE12 --> mais de doze horas após a adimissão\n",
        "\n",
        "Já para a coluna ICU (nossa variável target):\n",
        "- 0 ---> o paciente não foi internado na UTI naquela janela de tempo\n",
        "- 1 ---> o paciente foi internado na UTI naquela janela de tempo\n",
        "\n",
        "Segundo as instruções do Sírio Libanês, duas premissas básicas devem ser consideradas na resolução do desafio:\n",
        "- Não utilizar dados de uma linha para a qual a variável alvo é igual a 1. Isso é razoável, pois queremos prever quando um paciente irá para a UTI com antecedêcia, então não faz sentido alimentar o modelo com dados de um momento em que o evento que ele deveria prever já ocorreu. \n",
        "- Quanto maior a antecedência da previsão, melhor. Por isso, tomamos a decisão, no notebook anterior, de utilizar apenas dados da primeira janela de tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idg0MpAL4bMa"
      },
      "source": [
        "# Importação das bibliotecas e funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMvgWB2TQQLX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import warnings\n",
        "import math\n",
        "import matplotlib.ticker as ticker\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxaFFS0JOeXJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import tree "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oXxPRyeTgZH"
      },
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LbhLLbsOYB6"
      },
      "source": [
        "def roda_modelo_cv(modelo, dados, x,y, n_splits = 5, n_repeats = 10, tipo_score = 'roc_auc'):\n",
        "\n",
        "    np.random.seed(1231234)\n",
        "    dados = dados.sample(frac=1).reset_index(drop=True)\n",
        "    x_columns = dados.columns\n",
        "\n",
        "    cv = RepeatedStratifiedKFold(n_splits = n_splits, n_repeats=n_repeats, random_state=527435)\n",
        "    #resultados=cross_validate(modelo, x, y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "    resultados=cross_validate(modelo, x, y, cv=cv, scoring= tipo_score, return_train_score=True)\n",
        "\n",
        "    auc_medio_teste = np.mean(resultados['test_score'])\n",
        "    auc_medio_treino = np.mean(resultados['train_score'])\n",
        "\n",
        "    auc_teste_std = np.std(resultados['test_score'])\n",
        "    auc_treino_std = np.std(resultados['train_score'])\n",
        "    \n",
        "    linf_teste = auc_medio_teste - 2*auc_teste_std\n",
        "    lsup_teste = auc_medio_teste + 2*auc_teste_std\n",
        "    \n",
        "    linf_treino = auc_medio_treino - 2*auc_treino_std\n",
        "    lsup_treino = auc_medio_treino + 2*auc_treino_std\n",
        "    \n",
        "\n",
        "    print(f'{tipo_score} médios \\n |  Teste --- Treino | \\n | {round(100*auc_medio_teste,2)}% ---- {round(100*auc_medio_treino,2)}% |')\n",
        "    print(f'Intervalo de 2 desvios padrões \\n | Teste -------------- Treino | \\n | ({round(100*linf_teste, 2)}%, {round(100*lsup_teste, 2)}%) -- ({round(100*linf_treino, 2)}%, {round(100*lsup_treino, 2)}%)|')\n",
        "    #return auc_medio, auc_medio_treino\n",
        "   "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTXiPJBiqZJ1"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz48A5Mqd0QR"
      },
      "source": [
        "def compara_lista_series_binarias(lista, tolerancia = 25, prob = False):\n",
        "  if len(set([len(x) for x in lista])) > 1:\n",
        "    print('ERRO: séries com comprimentos distintos')\n",
        "    return False \n",
        "\n",
        "  n = len(lista[0])\n",
        "  tl = min([tolerancia, n-1])\n",
        "  resultado = []\n",
        "\n",
        "  for i in range(0,n):\n",
        "    soma = 0\n",
        "    for j in range(0,len(lista)):\n",
        "      soma = soma + lista[j][i]\n",
        "    if prob == False:\n",
        "      if soma > tl:\n",
        "        resultado.append(1)\n",
        "      else:\n",
        "        resultado.append(0)\n",
        "    else:\n",
        "      resultado.append(soma/len(lista))\n",
        "\n",
        "  return resultado"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9N9Mf4AHzue"
      },
      "source": [
        "def analisa_prob_series(lista, corte = 'auto', percentil = 0.25):\n",
        "  if len(lista) != 3:\n",
        "    print('Erro! Lista deve conter 3 séries')\n",
        "    return False\n",
        "  if len(lista[0]) != len(lista[1]):\n",
        "    print('Erro! As duas primeiras séries devem ter mesmo comprimento')\n",
        "    return False\n",
        "  \n",
        "  if isinstance(corte, int):\n",
        "    corte = float(corte)\n",
        "\n",
        "  if corte == 'auto':\n",
        "    dict_prob = {'Real': lista[0], 'Prob' : lista[1]}\n",
        "    df_treino = pd.DataFrame(dict_prob)\n",
        "    corte = df_treino.query('Real == 1')['Prob'].quantile(percentil)\n",
        "    q0 = df_treino.query('Real == 0')['Prob'].quantile(1)\n",
        "    print(f'O máximo do 0 é {q0}')\n",
        "    #if q0 < corte:\n",
        "      #corte = q0\n",
        "\n",
        "  if isinstance(corte, float) == False:\n",
        "    print('Erro! Valor de corte não identificado')\n",
        "    return False\n",
        "  \n",
        "  if corte > 1 or corte < 0:\n",
        "    print('Erro! Valor de corte deve ser um float entre 0 e 1 ou \"auto\"')\n",
        "\n",
        "  resultado = []\n",
        "  A = lista[2]\n",
        "  n = len(A)\n",
        "  print(f'Valor de corte = {corte}')\n",
        "\n",
        "  for i in range(0,n):\n",
        "    if A[i] >= corte:\n",
        "      resultado.append(1)\n",
        "    else:\n",
        "      resultado.append(0)\n",
        "\n",
        "  return resultado"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbO7usFHEXeb"
      },
      "source": [
        "def modelo_repetido(estimador, X_train, y_train, X_test, y_test, n_rep = 10, n_folds = 5,\n",
        "                    tol = 25, tol_overfit = 0.95, prob = False, corte = 'auto', percentil = 0.25, verbosity = 1):\n",
        "  \n",
        "  from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "  from sklearn.metrics import accuracy_score, f1_score,recall_score, precision_score, roc_auc_score\n",
        "\n",
        "  kfold = RepeatedStratifiedKFold(n_splits= n_folds, n_repeats= n_rep, random_state=527435)\n",
        "  lista_pred_teste = []\n",
        "  lista_pred_treino = []\n",
        "  cont = 0\n",
        "  cont_overfit = 0\n",
        "  for train_index, test_index in kfold.split(X_train, y_train):\n",
        "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "    estimador.fit(X_train_fold, y_train_fold)\n",
        "    \n",
        "    acc_treino = accuracy_score(y_train_fold,estimador.predict(X_train_fold))\n",
        "    if acc_treino < tol_overfit:\n",
        "      acc_teste = accuracy_score(y_test_fold, estimador.predict(X_test_fold))\n",
        "      y_pred_teste = estimador.predict(X_test)\n",
        "      lista_pred_teste.append(y_pred_teste.tolist())\n",
        "      y_pred_treino = estimador.predict(X_train)\n",
        "      lista_pred_treino.append(y_pred_treino)\n",
        "      cont = cont + 1\n",
        "    else:\n",
        "      cont_overfit = cont_overfit +1\n",
        "  \n",
        "  n = len(y_test)\n",
        "  if cont == 0:\n",
        "    print('Não foram encontados modelos sem overfitting')\n",
        "    return False, False\n",
        "\n",
        "  if verbosity != 0:\n",
        "    print(f'Foram realizadas {n_folds*n_rep} iterações')\n",
        "    print(f'Serão utilizados {cont} modelos no processo')\n",
        "    print(f'Um total de {cont_overfit} modelos tiveram overfitting com os dados de treino')\n",
        "\n",
        "  previsao_teste = compara_lista_series_binarias(lista_pred_teste, tolerancia= tol, prob = prob)\n",
        "  previsao_treino = compara_lista_series_binarias(lista_pred_treino, tolerancia = tol, prob = prob)\n",
        "  if prob == False:\n",
        "    previsao = [previsao_teste, previsao_treino]\n",
        "\n",
        "    if verbosity != 0:\n",
        "      print('\\n || Relatório de classificação (dados de teste) || \\n')\n",
        "      print(classification_report(y_test, previsao_teste))\n",
        "      print('\\n || Relatório de classificação (dados de treino) || \\n')\n",
        "      print(classification_report(y_train, previsao_treino)) \n",
        "\n",
        "    acc = [round(100*accuracy_score(y_test,previsao_teste),2), round(100*accuracy_score(y_treino,previsao_treino),2)]\n",
        "    prec = [round(100*precision_score(y_test,previsao_teste),2), round(100*precision_score(y_train,previsao_treino),2)]\n",
        "    recall = [round(100*recall_score(y_test,previsao_teste),2), round(100*recall_score(y_train,previsao_treino),2)]\n",
        "    f1 = [round(100*f1_score(y_test,previsao_teste),2), round(100*f1_score(y_train,previsao_treino),2)]\n",
        "  else:\n",
        "    previsao_y = analisa_prob_series([y_train.tolist(), previsao_treino, previsao_teste], corte = corte, percentil= percentil)\n",
        "    if verbosity != 0:\n",
        "      print('\\n || Relatório de classificação (dados de teste) || \\n')\n",
        "      print(classification_report(y_test, previsao_y))\n",
        "    acc = round(100*accuracy_score(y_test,previsao_y),2)\n",
        "    prec = round(100*precision_score(y_test,previsao_y),2)\n",
        "    recall = round(100*recall_score(y_test,previsao_y),2)\n",
        "    f1 = round(100*f1_score(y_test,previsao_y),2)\n",
        "    previsao = previsao_y\n",
        "\n",
        "\n",
        "  dict_scores = {\n",
        "      'Acuracia': acc,\n",
        "      'Precisão' : prec,\n",
        "      'Revocação' : recall,\n",
        "      'F1' : f1\n",
        "                 }\n",
        "\n",
        "  return previsao, dict_scores"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRxgMrAgMYeh"
      },
      "source": [
        "# Importação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU4SW5RoqIl8"
      },
      "source": [
        "Vamos importar o dataframe criado no notebook anterior. Ele foi guardado na pasta Dados do repositório desse projeto, no Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQSmdrRlMauA"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/renanmath/Bootcamp_Projeto_Final/main/Dados/dados_resumidos_UTI'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gqwdI27DMehT",
        "outputId": "f2000458-c473-4428-b932-1f9cd0960a35"
      },
      "source": [
        "dados_resumidos = pd.read_csv(url)\n",
        "dados_resumidos.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PATIENT_VISIT_IDENTIFIER</th>\n",
              "      <th>AGE_ABOVE65</th>\n",
              "      <th>AGE_PERCENTIL</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>DISEASE GROUPING 1</th>\n",
              "      <th>DISEASE GROUPING 2</th>\n",
              "      <th>DISEASE GROUPING 3</th>\n",
              "      <th>DISEASE GROUPING 4</th>\n",
              "      <th>DISEASE GROUPING 5</th>\n",
              "      <th>DISEASE GROUPING 6</th>\n",
              "      <th>HTN</th>\n",
              "      <th>IMMUNOCOMPROMISED</th>\n",
              "      <th>OTHER</th>\n",
              "      <th>BIC_VENOUS_MEDIAN</th>\n",
              "      <th>BIC_VENOUS_MEAN</th>\n",
              "      <th>BIC_VENOUS_MIN</th>\n",
              "      <th>BIC_VENOUS_MAX</th>\n",
              "      <th>CALCIUM_MEDIAN</th>\n",
              "      <th>CALCIUM_MEAN</th>\n",
              "      <th>CALCIUM_MIN</th>\n",
              "      <th>CALCIUM_MAX</th>\n",
              "      <th>GLUCOSE_MEDIAN</th>\n",
              "      <th>GLUCOSE_MEAN</th>\n",
              "      <th>GLUCOSE_MIN</th>\n",
              "      <th>GLUCOSE_MAX</th>\n",
              "      <th>HEMATOCRITE_MEDIAN</th>\n",
              "      <th>HEMATOCRITE_MEAN</th>\n",
              "      <th>HEMATOCRITE_MIN</th>\n",
              "      <th>HEMATOCRITE_MAX</th>\n",
              "      <th>HEMOGLOBIN_MEDIAN</th>\n",
              "      <th>HEMOGLOBIN_MEAN</th>\n",
              "      <th>HEMOGLOBIN_MIN</th>\n",
              "      <th>HEMOGLOBIN_MAX</th>\n",
              "      <th>LACTATE_MEDIAN</th>\n",
              "      <th>LACTATE_MEAN</th>\n",
              "      <th>LACTATE_MIN</th>\n",
              "      <th>LACTATE_MAX</th>\n",
              "      <th>LEUKOCYTES_MEDIAN</th>\n",
              "      <th>LEUKOCYTES_MEAN</th>\n",
              "      <th>LEUKOCYTES_MIN</th>\n",
              "      <th>...</th>\n",
              "      <th>POTASSIUM_MEAN</th>\n",
              "      <th>POTASSIUM_MIN</th>\n",
              "      <th>POTASSIUM_MAX</th>\n",
              "      <th>SAT02_VENOUS_MEDIAN</th>\n",
              "      <th>SAT02_VENOUS_MEAN</th>\n",
              "      <th>SAT02_VENOUS_MIN</th>\n",
              "      <th>SAT02_VENOUS_MAX</th>\n",
              "      <th>SODIUM_MEDIAN</th>\n",
              "      <th>SODIUM_MEAN</th>\n",
              "      <th>SODIUM_MIN</th>\n",
              "      <th>SODIUM_MAX</th>\n",
              "      <th>UREA_MEDIAN</th>\n",
              "      <th>UREA_MEAN</th>\n",
              "      <th>UREA_MIN</th>\n",
              "      <th>UREA_MAX</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEAN</th>\n",
              "      <th>HEART_RATE_MEAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEAN</th>\n",
              "      <th>TEMPERATURE_MEAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEDIAN</th>\n",
              "      <th>HEART_RATE_MEDIAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEDIAN</th>\n",
              "      <th>TEMPERATURE_MEDIAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MIN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MIN</th>\n",
              "      <th>HEART_RATE_MIN</th>\n",
              "      <th>RESPIRATORY_RATE_MIN</th>\n",
              "      <th>TEMPERATURE_MIN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MAX</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MAX</th>\n",
              "      <th>HEART_RATE_MAX</th>\n",
              "      <th>RESPIRATORY_RATE_MAX</th>\n",
              "      <th>TEMPERATURE_MAX</th>\n",
              "      <th>OXYGEN_SATURATION_MAX</th>\n",
              "      <th>WINDOW</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.237113</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.162393</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.459459</td>\n",
              "      <td>-0.432836</td>\n",
              "      <td>-0.636364</td>\n",
              "      <td>-0.420290</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0-2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.489712</td>\n",
              "      <td>-0.685470</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.645951</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.935673</td>\n",
              "      <td>-0.506173</td>\n",
              "      <td>-0.815385</td>\n",
              "      <td>-0.056604</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>-0.525773</td>\n",
              "      <td>-0.5125</td>\n",
              "      <td>-0.111111</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>-0.435897</td>\n",
              "      <td>-0.491892</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>0.101449</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0-2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>0.012346</td>\n",
              "      <td>-0.369231</td>\n",
              "      <td>-0.528302</td>\n",
              "      <td>-0.457627</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.012346</td>\n",
              "      <td>-0.369231</td>\n",
              "      <td>-0.528302</td>\n",
              "      <td>-0.448276</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.175258</td>\n",
              "      <td>-0.1125</td>\n",
              "      <td>-0.384615</td>\n",
              "      <td>-0.357143</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>-0.299145</td>\n",
              "      <td>-0.556757</td>\n",
              "      <td>-0.626866</td>\n",
              "      <td>-0.515152</td>\n",
              "      <td>-0.420290</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0-2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>0.160377</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>0.160377</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.443299</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.196581</td>\n",
              "      <td>-0.571429</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.351351</td>\n",
              "      <td>-0.044776</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0-2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.538462</td>\n",
              "      <td>-0.537736</td>\n",
              "      <td>-0.525424</td>\n",
              "      <td>-0.196429</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.538462</td>\n",
              "      <td>-0.537736</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>-0.196429</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>-0.3750</td>\n",
              "      <td>-0.401709</td>\n",
              "      <td>-0.428571</td>\n",
              "      <td>0.252747</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.567568</td>\n",
              "      <td>-0.626866</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0-2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   PATIENT_VISIT_IDENTIFIER  AGE_ABOVE65  ...  WINDOW  ICU\n",
              "0                         0            1  ...     0-2    1\n",
              "2                         2            0  ...     0-2    1\n",
              "3                         3            0  ...     0-2    0\n",
              "4                         4            0  ...     0-2    0\n",
              "5                         5            0  ...     0-2    0\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRQNYNOZPahq"
      },
      "source": [
        "Não precisaremos das colunas 'PATIENT_VISIT_IDENTIFIER' e 'WINDOW'. Além disso, visto que 'AGE_PERCENTIL' já sumariza muito bem as informações sobre idade, creio que 'AGE_ABOVE65' seja supérfuo nesse caso. Por fim, não usaremos 'GENDER' por questões éticas. Assim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DOki9xTMQCRJ",
        "outputId": "616e6ff6-b65a-4ab1-9ce2-f479fd3b9b77"
      },
      "source": [
        "dados_limpos = dados_resumidos.drop(['PATIENT_VISIT_IDENTIFIER', 'WINDOW', 'AGE_ABOVE65', 'GENDER'], axis = 1)\n",
        "dados_limpos.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE_PERCENTIL</th>\n",
              "      <th>DISEASE GROUPING 1</th>\n",
              "      <th>DISEASE GROUPING 2</th>\n",
              "      <th>DISEASE GROUPING 3</th>\n",
              "      <th>DISEASE GROUPING 4</th>\n",
              "      <th>DISEASE GROUPING 5</th>\n",
              "      <th>DISEASE GROUPING 6</th>\n",
              "      <th>HTN</th>\n",
              "      <th>IMMUNOCOMPROMISED</th>\n",
              "      <th>OTHER</th>\n",
              "      <th>BIC_VENOUS_MEDIAN</th>\n",
              "      <th>BIC_VENOUS_MEAN</th>\n",
              "      <th>BIC_VENOUS_MIN</th>\n",
              "      <th>BIC_VENOUS_MAX</th>\n",
              "      <th>CALCIUM_MEDIAN</th>\n",
              "      <th>CALCIUM_MEAN</th>\n",
              "      <th>CALCIUM_MIN</th>\n",
              "      <th>CALCIUM_MAX</th>\n",
              "      <th>GLUCOSE_MEDIAN</th>\n",
              "      <th>GLUCOSE_MEAN</th>\n",
              "      <th>GLUCOSE_MIN</th>\n",
              "      <th>GLUCOSE_MAX</th>\n",
              "      <th>HEMATOCRITE_MEDIAN</th>\n",
              "      <th>HEMATOCRITE_MEAN</th>\n",
              "      <th>HEMATOCRITE_MIN</th>\n",
              "      <th>HEMATOCRITE_MAX</th>\n",
              "      <th>HEMOGLOBIN_MEDIAN</th>\n",
              "      <th>HEMOGLOBIN_MEAN</th>\n",
              "      <th>HEMOGLOBIN_MIN</th>\n",
              "      <th>HEMOGLOBIN_MAX</th>\n",
              "      <th>LACTATE_MEDIAN</th>\n",
              "      <th>LACTATE_MEAN</th>\n",
              "      <th>LACTATE_MIN</th>\n",
              "      <th>LACTATE_MAX</th>\n",
              "      <th>LEUKOCYTES_MEDIAN</th>\n",
              "      <th>LEUKOCYTES_MEAN</th>\n",
              "      <th>LEUKOCYTES_MIN</th>\n",
              "      <th>LEUKOCYTES_MAX</th>\n",
              "      <th>LINFOCITOS_MEDIAN</th>\n",
              "      <th>LINFOCITOS_MEAN</th>\n",
              "      <th>...</th>\n",
              "      <th>POTASSIUM_MEDIAN</th>\n",
              "      <th>POTASSIUM_MEAN</th>\n",
              "      <th>POTASSIUM_MIN</th>\n",
              "      <th>POTASSIUM_MAX</th>\n",
              "      <th>SAT02_VENOUS_MEDIAN</th>\n",
              "      <th>SAT02_VENOUS_MEAN</th>\n",
              "      <th>SAT02_VENOUS_MIN</th>\n",
              "      <th>SAT02_VENOUS_MAX</th>\n",
              "      <th>SODIUM_MEDIAN</th>\n",
              "      <th>SODIUM_MEAN</th>\n",
              "      <th>SODIUM_MIN</th>\n",
              "      <th>SODIUM_MAX</th>\n",
              "      <th>UREA_MEDIAN</th>\n",
              "      <th>UREA_MEAN</th>\n",
              "      <th>UREA_MIN</th>\n",
              "      <th>UREA_MAX</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEAN</th>\n",
              "      <th>HEART_RATE_MEAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEAN</th>\n",
              "      <th>TEMPERATURE_MEAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEDIAN</th>\n",
              "      <th>HEART_RATE_MEDIAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEDIAN</th>\n",
              "      <th>TEMPERATURE_MEDIAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MIN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MIN</th>\n",
              "      <th>HEART_RATE_MIN</th>\n",
              "      <th>RESPIRATORY_RATE_MIN</th>\n",
              "      <th>TEMPERATURE_MIN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MAX</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MAX</th>\n",
              "      <th>HEART_RATE_MAX</th>\n",
              "      <th>RESPIRATORY_RATE_MAX</th>\n",
              "      <th>TEMPERATURE_MAX</th>\n",
              "      <th>OXYGEN_SATURATION_MAX</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.090147</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.835844</td>\n",
              "      <td>-0.914938</td>\n",
              "      <td>-0.914938</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>-0.518519</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.028571</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.237113</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.162393</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.459459</td>\n",
              "      <td>-0.432836</td>\n",
              "      <td>-0.636364</td>\n",
              "      <td>-0.420290</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>-0.780261</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.144654</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.382773</td>\n",
              "      <td>-0.908714</td>\n",
              "      <td>-0.908714</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>-0.703704</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.836145</td>\n",
              "      <td>-0.489712</td>\n",
              "      <td>-0.685470</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.645951</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.935673</td>\n",
              "      <td>-0.506173</td>\n",
              "      <td>-0.815385</td>\n",
              "      <td>-0.056604</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>-0.525773</td>\n",
              "      <td>-0.5125</td>\n",
              "      <td>-0.111111</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>-0.435897</td>\n",
              "      <td>-0.491892</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>0.101449</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.203354</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.219512</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.828421</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.729239</td>\n",
              "      <td>-0.836100</td>\n",
              "      <td>-0.836100</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>-0.937349</td>\n",
              "      <td>0.012346</td>\n",
              "      <td>-0.369231</td>\n",
              "      <td>-0.528302</td>\n",
              "      <td>-0.457627</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.012346</td>\n",
              "      <td>-0.369231</td>\n",
              "      <td>-0.528302</td>\n",
              "      <td>-0.448276</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.175258</td>\n",
              "      <td>-0.1125</td>\n",
              "      <td>-0.384615</td>\n",
              "      <td>-0.357143</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>-0.299145</td>\n",
              "      <td>-0.556757</td>\n",
              "      <td>-0.626866</td>\n",
              "      <td>-0.515152</td>\n",
              "      <td>-0.420290</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>-0.851024</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>0.304878</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.702202</td>\n",
              "      <td>-0.641079</td>\n",
              "      <td>-0.641079</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>-0.592593</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>-0.903614</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>0.160377</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>0.160377</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.443299</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.196581</td>\n",
              "      <td>-0.571429</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.351351</td>\n",
              "      <td>-0.044776</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>-0.891993</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.291405</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.706450</td>\n",
              "      <td>-0.340249</td>\n",
              "      <td>-0.340249</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.884337</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.538462</td>\n",
              "      <td>-0.537736</td>\n",
              "      <td>-0.525424</td>\n",
              "      <td>-0.196429</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.538462</td>\n",
              "      <td>-0.537736</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>-0.196429</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>-0.3750</td>\n",
              "      <td>-0.401709</td>\n",
              "      <td>-0.428571</td>\n",
              "      <td>0.252747</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.567568</td>\n",
              "      <td>-0.626866</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   AGE_PERCENTIL  DISEASE GROUPING 1  ...  OXYGEN_SATURATION_MAX  ICU\n",
              "0            0.6                 0.0  ...               0.736842    1\n",
              "2            0.1                 0.0  ...               1.000000    1\n",
              "3            0.4                 0.0  ...               0.684211    0\n",
              "4            0.1                 0.0  ...               0.894737    0\n",
              "5            0.1                 0.0  ...               0.842105    0\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUKsCc08qg0-"
      },
      "source": [
        "Esse será nosso dataframe final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOBCoAXOi5Y"
      },
      "source": [
        "# Modelo baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncFHg-1lqjnV"
      },
      "source": [
        "Antes de partirmos para os modelos digamos clássicos, vamos aproveitar as informações e insights que tiramos da nossa análise exploratória e criar um modelo 'manual'. Esse modelo servirá de baseline, no sentido que qualquer outro modelo que fizermos deverá ser melhor que este. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsJLwGZOrfhn"
      },
      "source": [
        "colunas_continuas = dados_limpos.columns.tolist()[10:-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCUszx41HJFM"
      },
      "source": [
        "def compara_series(serie1,serie2, tolerancia = 3):\n",
        "  if len(serie1) != len(serie2):\n",
        "    print('ERRO')\n",
        "    return False\n",
        "  \n",
        "  tl = tolerancia\n",
        "  n = len(serie1)\n",
        "  lista_aux = []\n",
        "\n",
        "  for i in range(0,n):\n",
        "    if serie1[i] > serie2[i]:\n",
        "      lista_aux.append(1)\n",
        "    else:\n",
        "      lista_aux.append(0)\n",
        "  \n",
        "  if sum(lista_aux) >= tl:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmbtbLaZFY-i"
      },
      "source": [
        "def modelo_simples(dataset, tolerancia = 10, p0 = 0.75, p1 = 0.75):\n",
        "  n = dataset.shape[0]\n",
        "  tl = tolerancia\n",
        "  yhat = []\n",
        "  A = [dataset.query('ICU == 0')[x].quantile(p0) for x in colunas_continuas]\n",
        "  B = [dataset.query('ICU == 1')[x].quantile(p1) for x in colunas_continuas]\n",
        "\n",
        "  for i in range(0,n):\n",
        "    a = compara_series(dataset.iloc[i][colunas_continuas].tolist(), A, tolerancia= tl)\n",
        "    b = compara_series(B,dataset.iloc[i][colunas_continuas].tolist(), tolerancia= tl)\n",
        "    if a+b == 2:\n",
        "      yhat.append(1)\n",
        "    else:\n",
        "      yhat.append(0)\n",
        "  \n",
        "  return yhat"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Q2_zVUZU-d"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc_l_McoZDa1"
      },
      "source": [
        "def analisa_modelos(modelo):\n",
        "  lista_scores_f1 = []\n",
        "  lista_scores_ac = []\n",
        "  max_ac, max_f1, tl_max_ac, tl_max_f1 = 0,0,0,0\n",
        "  for tl in range(1,50):\n",
        "    y_pred = modelo(dados_limpos, tolerancia= tl)\n",
        "    score_ac = accuracy_score(dados_limpos['ICU'].tolist(), y_pred)\n",
        "    lista_scores_ac.append(score_ac)\n",
        "    if score_ac > max_ac:\n",
        "      max_ac = score_ac\n",
        "      tl_max_ac = tl\n",
        "    score_f1 = f1_score(dados_limpos['ICU'].tolist(), y_pred)\n",
        "    lista_scores_f1.append(score_f1)\n",
        "    if score_f1 > max_f1:\n",
        "      max_f1 = score_f1\n",
        "      tl_max_f1 = tl\n",
        "\n",
        "  print(f'\\n O score f1 máximo foi {max_f1}, atingindo em tl = {tl_max_f1} \\n A acurácia máxima foi {max_ac} atinginda em tl = {tl_max_ac}')\n",
        "  return lista_scores_ac, lista_scores_f1, tl_max_ac, tl_max_f1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4AzNxUQfuik",
        "outputId": "42350fc1-f99a-403b-ad3d-fcec75d30a83"
      },
      "source": [
        "lista_scores_ac, lista_scores_f1, tl_max_ac, tl_max_f1 = analisa_modelos(modelo_simples)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " O score f1 máximo foi 0.6544622425629292, atingindo em tl = 12 \n",
            " A acurácia máxima foi 0.6438746438746439 atinginda em tl = 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eysKxBnaBnD"
      },
      "source": [
        "def plotar_grafico_scores(lista_scores_ac,lista_scores_f1):\n",
        "  plt.figure(figsize=(7,6))\n",
        "  sns.lineplot(y = lista_scores_ac, x = [x for x in range(1,50)], color = 'b')\n",
        "  sns.lineplot(y = lista_scores_f1, x = [x for x in range(1,50)], color = 'g')\n",
        "  plt.title('Gráfico tolerância x métricas', fontdict={'fontsize':15, 'fontweight':'bold'})\n",
        "  plt.xlabel('tolerância', fontdict={'fontsize':13, 'fontweight':'bold'})\n",
        "  plt.ylabel('Métricas', fontdict={'fontsize':13, 'fontweight':'bold'})\n",
        "  plt.grid()\n",
        "  plt.legend(['Acurácia', 'F-1'])\n",
        "  plt.show()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "n1bFT0vYhY0l",
        "outputId": "458b738c-ecb6-41d3-cbc8-29b8a460c3ed"
      },
      "source": [
        "plotar_grafico_scores(lista_scores_ac, lista_scores_f1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbMS2HpJATf"
      },
      "source": [
        "pred = modelo_simples(dados_limpos, tolerancia=tl_max_ac)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITLh3g5lJATg",
        "outputId": "50a8f76a-ad0c-43b9-bb77-08154f53818f"
      },
      "source": [
        "confusion_matrix(dados_limpos['ICU'].tolist(), pred)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[138,  51],\n",
              "       [ 74,  88]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLMtn3Q5JATh",
        "outputId": "87b29d12-a487-4940-9861-9b78ef2991b8"
      },
      "source": [
        "print(classification_report(dados_limpos['ICU'].tolist(), pred))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.73      0.69       189\n",
            "           1       0.63      0.54      0.58       162\n",
            "\n",
            "    accuracy                           0.64       351\n",
            "   macro avg       0.64      0.64      0.64       351\n",
            "weighted avg       0.64      0.64      0.64       351\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KywGkmHxt2Ag"
      },
      "source": [
        "Vemos então que para a tolerância que dá a acurácia máxima, temos uma acurácia de 64% o que não é de todo ruim, considerando que o modelo foi construindo 'à mão'. \n",
        "vejamos com o outro parâmetro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rpm4yeneB_6"
      },
      "source": [
        "pred = modelo_simples(dados_limpos, tolerancia= tl_max_f1)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJM7dUyUeB_7",
        "outputId": "f0c5680d-e431-4c1c-8cbd-580378996874"
      },
      "source": [
        "confusion_matrix(dados_limpos['ICU'].tolist(), pred)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 57, 132],\n",
              "       [ 19, 143]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryi3qk3EeB_7",
        "outputId": "cc064d40-89cb-4bc1-a171-785d0d8cb429"
      },
      "source": [
        "print(classification_report(dados_limpos['ICU'].tolist(), pred))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.30      0.43       189\n",
            "           1       0.52      0.88      0.65       162\n",
            "\n",
            "    accuracy                           0.57       351\n",
            "   macro avg       0.64      0.59      0.54       351\n",
            "weighted avg       0.64      0.57      0.53       351\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLy5sC3ufBe"
      },
      "source": [
        "Melhoramos o f1-score mas a acurácia ficou comprometida. Aqui vemos como temos um jogo de 'toma lá, dá cá': ao passo que melhoramos a precisão 0 e revocação de 1, pioramos sensivelmente a precisão de 1 e revocação de 0. Esse modelo não é de todo ruim para resolver o problema 2, mas é péssimo para resolver o problema 1.\n",
        "O modelo com o parâmetro anterior era mais equilibrado nesse quesito. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sebm3hd2vdUN"
      },
      "source": [
        "Mas já que conseguimos criar um modelo com acurácia de 64%, nada abaixo disso será aceitável. Até porque, essa métrica é horrível quando se trata de problemas relacionados a saúde pública. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XAehcnljtG"
      },
      "source": [
        "# Modelos intermediários"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V5N42LUv7mD"
      },
      "source": [
        "Passemos agora ao desenvolvimento de modelos digamos 'clássicos'. \n",
        "Escolhemos atacar o problema utilizando os seguintes modelos:\n",
        "- Regressão logísitica\n",
        "- Floresta aleatória\n",
        "- Vizinhos mais próximos\n",
        "- XGBoost\n",
        "- GradientBoosting \n",
        "- MLP (Multilayer perceptron)\n",
        "\n",
        "Para cada um desses modelos, utilizamos um GridSearch para selecionar bons hiperparâmetros. A implementação dos GridSearch não foi feita nesse notebook, mas em três notebook auxiliares, também disponíveis no repositório do Github. Aqui utilizaremos apenas os resultados dessas análises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIZa-ugUxdMC"
      },
      "source": [
        "Para cada um desses modelos, o procedimento utilizado foi o mesmo:\n",
        "- Foi criado um modelo base com os hiperparâmetros coletados da análise com GridSearch.\n",
        "- Utilizamos uma função que roda esse modelo várias vezes, utilizando um RepeatedStratifiedKFold pro debaixo dos panos. \n",
        "- Fizemos uma busca por melhores hiperparâmetros para essa função. \n",
        "- Comparamos as métricas em todos os passos. \n",
        "\n",
        "Em todos esses passos, sempre que possível, utilizamos um random_state = 527435, para garantir a reprodutibilidade dos resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxpc-N87yRjs"
      },
      "source": [
        "Sem mais delongas, eis as análises:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAXRzIDOkqI"
      },
      "source": [
        "## Criação de variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK3yl_ysOzWA"
      },
      "source": [
        "features_continuas = dados_limpos.columns[10:-1].tolist()\n",
        "features_categoricas = dados_limpos.columns[0:9].tolist()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWLYGc-6RwoK"
      },
      "source": [
        "X = dados_limpos.drop('ICU', axis = 1)\n",
        "y = dados_limpos['ICU']"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdXNX6gXR9A4"
      },
      "source": [
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X,y, test_size = 0.25, stratify = y, random_state = 527435)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wl36LrlxjyA"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9rmXA-pEha"
      },
      "source": [
        "modelo_reg_log = LogisticRegression(solver = 'lbfgs',\n",
        "                                    C = 1.0,\n",
        "                                    class_weight = None,\n",
        "                                    fit_intercept = True,\n",
        "                                    max_iter = 50,\n",
        "                                    tol = 0.1,\n",
        "                                    random_state = 527435)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbOX54_vsQ29",
        "outputId": "5c117ec0-6d93-4fe0-9ac6-3453e56a16b4"
      },
      "source": [
        "modelo_reg_log.fit(X_treino, y_treino)\n",
        "y_pred_reg_log = modelo_reg_log.predict(X_teste)\n",
        "acur = modelo_reg_log.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia = 67.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwhJTIR6sQ3A",
        "outputId": "b815e842-e1ab-4479-ad25-b40e54d80cf2"
      },
      "source": [
        "print(classification_report(y_treino,modelo_reg_log.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       142\n",
            "           1       0.78      0.75      0.77       121\n",
            "\n",
            "    accuracy                           0.79       263\n",
            "   macro avg       0.79      0.79      0.79       263\n",
            "weighted avg       0.79      0.79      0.79       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMo1-wR_sQ2_",
        "outputId": "3242f7bd-d99f-4ef9-93f6-b7a74ab1f8a8"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_reg_log))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.70      0.69        47\n",
            "           1       0.65      0.63      0.64        41\n",
            "\n",
            "    accuracy                           0.67        88\n",
            "   macro avg       0.67      0.67      0.67        88\n",
            "weighted avg       0.67      0.67      0.67        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntq1AmoAynbl"
      },
      "source": [
        "Temos uma acurácia de 67%. Ou seja, não muito melhor que nosso modelo baseline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A4t40rGhWvX",
        "outputId": "a50685b4-9c9d-4183-8546-0041db047655"
      },
      "source": [
        "y_previsao_reg_log, scores_reg_log = modelo_repetido(modelo_reg_log,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 100,n_folds = 10, tol= 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 1000 iterações\n",
            "Serão utilizados 1000 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.57      0.68        47\n",
            "           1       0.64      0.85      0.73        41\n",
            "\n",
            "    accuracy                           0.70        88\n",
            "   macro avg       0.73      0.71      0.70        88\n",
            "weighted avg       0.73      0.70      0.70        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.65      0.73       142\n",
            "           1       0.67      0.85      0.75       121\n",
            "\n",
            "    accuracy                           0.74       263\n",
            "   macro avg       0.75      0.75      0.74       263\n",
            "weighted avg       0.76      0.74      0.74       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A22uxGWoy0J7"
      },
      "source": [
        "Conseguimos subir a acurácia para 70%, o que já é uma melhora considerável em relação ao baseline. Comparando as precisões e as revocações, vemos que parece um bom modelo para lidar com o problema 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjLDKTRwSUJo",
        "outputId": "158c9d5a-aeee-4dd0-8209-b2be783e43f1"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_reg_log[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27 20]\n",
            " [ 6 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8AUODAYpXQh"
      },
      "source": [
        "Vamos agora analizar os parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ZPVnDoQFzzaX",
        "outputId": "d53f2043-0162-4c84-de88-82d3fb1c48a4"
      },
      "source": [
        "import itertools\n",
        "df_scores_log_reg = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10*x for x in range(1,11)]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  y_previsao, scores_reg_log = modelo_repetido(LogisticRegression(solver= 'liblinear', tol=1.0, C = 1.0, max_iter= 50),\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0)\n",
        "  ac = scores_reg_log['Acuracia'][0]\n",
        "  pr = scores_reg_log['Precisão'][0]\n",
        "  rv = scores_reg_log['Revocação'][0]\n",
        "  f1 = scores_reg_log['F1'][0]\n",
        "  nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "  df_scores_log_reg.loc[ind] = nova_coluna\n",
        "  ind = ind+1\n",
        "  \n",
        "df_scores_log_reg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>62.26</td>\n",
              "      <td>80.49</td>\n",
              "      <td>70.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>61.54</td>\n",
              "      <td>78.05</td>\n",
              "      <td>68.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>62.50</td>\n",
              "      <td>73.17</td>\n",
              "      <td>67.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>61.11</td>\n",
              "      <td>80.49</td>\n",
              "      <td>69.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>61.54</td>\n",
              "      <td>78.05</td>\n",
              "      <td>68.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>90.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>68.29</td>\n",
              "      <td>66.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>68.29</td>\n",
              "      <td>66.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>68.29</td>\n",
              "      <td>66.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>68.29</td>\n",
              "      <td>66.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>68.29</td>\n",
              "      <td>66.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>330 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0          10.0    5.0         0.0     68.18     62.26      80.49  70.21\n",
              "1          10.0    7.0         0.0     67.05     61.54      78.05  68.82\n",
              "2          10.0   10.0         0.0     67.05     62.50      73.17  67.42\n",
              "3          20.0    5.0         0.0     67.05     61.11      80.49  69.47\n",
              "4          20.0    7.0         0.0     67.05     61.54      78.05  68.82\n",
              "..          ...    ...         ...       ...       ...        ...    ...\n",
              "325        90.0    7.0       100.0     68.18     65.12      68.29  66.67\n",
              "326        90.0   10.0       100.0     68.18     65.12      68.29  66.67\n",
              "327       100.0    5.0       100.0     68.18     65.12      68.29  66.67\n",
              "328       100.0    7.0       100.0     68.18     65.12      68.29  66.67\n",
              "329       100.0   10.0       100.0     68.18     65.12      68.29  66.67\n",
              "\n",
              "[330 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "mqKxMyDMUN3O",
        "outputId": "40794f56-4192-4684-f2d8-486e97376311"
      },
      "source": [
        "df_scores_log_reg.groupby(by = 'Repetições').max().sort_values(by = 'Acurácia', ascending = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Repetições</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>72.73</td>\n",
              "      <td>72.97</td>\n",
              "      <td>80.49</td>\n",
              "      <td>70.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>72.73</td>\n",
              "      <td>72.97</td>\n",
              "      <td>80.49</td>\n",
              "      <td>69.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>68.29</td>\n",
              "      <td>80.49</td>\n",
              "      <td>68.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>68.29</td>\n",
              "      <td>80.49</td>\n",
              "      <td>68.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>65.96</td>\n",
              "      <td>80.49</td>\n",
              "      <td>70.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>65.22</td>\n",
              "      <td>80.49</td>\n",
              "      <td>68.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>65.22</td>\n",
              "      <td>80.49</td>\n",
              "      <td>69.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>65.22</td>\n",
              "      <td>80.49</td>\n",
              "      <td>70.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>65.22</td>\n",
              "      <td>80.49</td>\n",
              "      <td>70.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>65.12</td>\n",
              "      <td>80.49</td>\n",
              "      <td>68.89</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "Repetições                                                         \n",
              "10.0         10.0       100.0     72.73     72.97      80.49  70.21\n",
              "20.0         10.0       100.0     72.73     72.97      80.49  69.47\n",
              "30.0         10.0       100.0     70.45     68.29      80.49  68.82\n",
              "40.0         10.0       100.0     70.45     68.29      80.49  68.82\n",
              "70.0         10.0       100.0     70.45     65.96      80.49  70.45\n",
              "60.0         10.0       100.0     69.32     65.22      80.49  68.97\n",
              "80.0         10.0       100.0     69.32     65.22      80.49  69.66\n",
              "90.0         10.0       100.0     69.32     65.22      80.49  70.33\n",
              "100.0        10.0       100.0     69.32     65.22      80.49  70.33\n",
              "50.0         10.0       100.0     68.18     65.12      80.49  68.89"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJi25_0jq1_u"
      },
      "source": [
        "Vemos então que os melhores parâmetros são:\n",
        "Repetições = 20\n",
        "Folds = 5\n",
        "Tolerância = 90\n",
        "\n",
        "Isso dá uma acurácia de 72.73%. Precisão e revocação (para positivos) também não estão ruins. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOlon5q_zp5D"
      },
      "source": [
        "Vejamos esse modelo na prática:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLaf1evmXP_M",
        "outputId": "e5300e49-3300-47bb-d744-3b84bf227edd"
      },
      "source": [
        "y_previsao, scores = modelo_repetido(LogisticRegression(solver= 'liblinear', tol=1.0, C = 1.0, max_iter= 50),\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 10, verbosity = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 50 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.68      0.70        47\n",
            "           1       0.65      0.68      0.67        41\n",
            "\n",
            "    accuracy                           0.68        88\n",
            "   macro avg       0.68      0.68      0.68        88\n",
            "weighted avg       0.68      0.68      0.68        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.76      0.76       142\n",
            "           1       0.72      0.73      0.72       121\n",
            "\n",
            "    accuracy                           0.75       263\n",
            "   macro avg       0.74      0.74      0.74       263\n",
            "weighted avg       0.75      0.75      0.75       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dre7D2Eiz2W2"
      },
      "source": [
        "Curiosamente, ao implentar o modelo, vemos uma leve queda nas métricas, apesar de termos tomado todas as medidas para garantir a reprodutibilidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LIl-7Pd4lqt"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SzxUzPx4uSd",
        "outputId": "0ae86a92-ea29-4f47-938d-f8ac5739faa9"
      },
      "source": [
        "modelo_forest = RandomForestClassifier(criterion= 'gini', max_depth= 15, max_features= 'log2',\n",
        "                                class_weight='balanced_subsample', max_samples = 0.6,\n",
        "                                random_state= 527435)\n",
        "modelo_forest.fit(X_treino, y_treino)\n",
        "y_pred_forest = modelo_forest.predict(X_teste)\n",
        "acur = modelo_forest.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia = 68.18%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A99GWwHe5aQe",
        "outputId": "bb5e8209-f82d-411a-e03d-9011d96b590b"
      },
      "source": [
        "print(classification_report(y_treino,modelo_forest.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       142\n",
            "           1       1.00      0.99      1.00       121\n",
            "\n",
            "    accuracy                           1.00       263\n",
            "   macro avg       1.00      1.00      1.00       263\n",
            "weighted avg       1.00      1.00      1.00       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LloDsYYr5aQr",
        "outputId": "67c1a5c1-bff4-4533-cdfc-a3aaf5d8d918"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_forest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.70      0.70        47\n",
            "           1       0.66      0.66      0.66        41\n",
            "\n",
            "    accuracy                           0.68        88\n",
            "   macro avg       0.68      0.68      0.68        88\n",
            "weighted avg       0.68      0.68      0.68        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D8v9CYr0GwV"
      },
      "source": [
        "O modelo Random Forest acabou sofrendo de um terrível overfitting. Em nosso modelo_repetido() será necessário afrouxar a tolerância para overfitting com os dados de teste, caso contrário não rodaremos nenhum modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOi0PiUn5wXn",
        "outputId": "7ca65af4-1254-4844-fa8e-d437fe0abab5"
      },
      "source": [
        "y_previsao_forest, scores_forest = modelo_repetido(modelo_forest,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0,\n",
        "                             tol_overfit = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 36 modelos no processo\n",
            "Um total de 14 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.47      0.62        47\n",
            "           1       0.61      0.95      0.74        41\n",
            "\n",
            "    accuracy                           0.69        88\n",
            "   macro avg       0.76      0.71      0.68        88\n",
            "weighted avg       0.77      0.69      0.68        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.64      0.78       142\n",
            "           1       0.70      1.00      0.83       121\n",
            "\n",
            "    accuracy                           0.81       263\n",
            "   macro avg       0.85      0.82      0.80       263\n",
            "weighted avg       0.86      0.81      0.80       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqHYlBbk0fR2"
      },
      "source": [
        "Notamos agora que melhoramos um pouco a questão do overfitting, porém o resultado com os dados de teste não foram tão satisfatórios. Mas esse modelo se mostra muito bom para tratar do problema 2: ele praticamente acerta todos os pacientes que não necessitam de UTI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc6kCOwG6c2t",
        "outputId": "31959a0b-8236-40fd-a6bf-22f8d076baee"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_forest[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[22 25]\n",
            " [ 2 39]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_NziJ-E7JDz",
        "outputId": "5c18ecac-8d5c-4fb0-a0f7-cee575fdd645"
      },
      "source": [
        "print(confusion_matrix(y_treino,y_previsao_forest[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 91  51]\n",
            " [  0 121]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeLCCxwx7zAM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f2ca13b2-d357-46d6-d6fc-31a998d5c1f2"
      },
      "source": [
        "import itertools\n",
        "df_scores_forest = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10,50,100]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  y_previsao_forest, scores_forest = modelo_repetido(modelo_forest,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0,\n",
        "                             tol_overfit = 1)\n",
        "  if scores_forest == False:\n",
        "    pass\n",
        "  else:\n",
        "    ac = scores_forest['Acuracia'][0]\n",
        "    pr = scores_forest['Precisão'][0]\n",
        "    rv = scores_forest['Revocação'][0]\n",
        "    f1 = scores_forest['F1'][0]\n",
        "    nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "    df_scores_forest.loc[ind] = nova_coluna\n",
        "    ind = ind+1\n",
        "  \n",
        "df_scores_forest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>60.94</td>\n",
              "      <td>95.12</td>\n",
              "      <td>74.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>60.66</td>\n",
              "      <td>90.24</td>\n",
              "      <td>72.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>59.68</td>\n",
              "      <td>90.24</td>\n",
              "      <td>71.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>60.61</td>\n",
              "      <td>97.56</td>\n",
              "      <td>74.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>60.00</td>\n",
              "      <td>95.12</td>\n",
              "      <td>73.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>65.91</td>\n",
              "      <td>70.73</td>\n",
              "      <td>68.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>62.50</td>\n",
              "      <td>73.17</td>\n",
              "      <td>67.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.18</td>\n",
              "      <td>63.27</td>\n",
              "      <td>75.61</td>\n",
              "      <td>68.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>64.00</td>\n",
              "      <td>78.05</td>\n",
              "      <td>70.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>64.00</td>\n",
              "      <td>78.05</td>\n",
              "      <td>70.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0         10.0    5.0         0.0     69.32     60.94      95.12  74.29\n",
              "1         10.0    7.0         0.0     68.18     60.66      90.24  72.55\n",
              "2         10.0   10.0         0.0     67.05     59.68      90.24  71.84\n",
              "3         50.0    5.0         0.0     69.32     60.61      97.56  74.77\n",
              "4         50.0    7.0         0.0     68.18     60.00      95.12  73.58\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "94        50.0    7.0       100.0     69.32     65.91      70.73  68.24\n",
              "95        50.0   10.0       100.0     67.05     62.50      73.17  67.42\n",
              "96       100.0    5.0       100.0     68.18     63.27      75.61  68.89\n",
              "97       100.0    7.0       100.0     69.32     64.00      78.05  70.33\n",
              "98       100.0   10.0       100.0     69.32     64.00      78.05  70.33\n",
              "\n",
              "[99 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_KZfePXK608B",
        "outputId": "1eaf1fab-c6a3-46a0-e108-def42bd68948"
      },
      "source": [
        "df_scores_forest.sort_values(by = 'Acurácia', ascending= False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.59</td>\n",
              "      <td>65.38</td>\n",
              "      <td>82.93</td>\n",
              "      <td>73.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>71.59</td>\n",
              "      <td>65.38</td>\n",
              "      <td>82.93</td>\n",
              "      <td>73.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>63.64</td>\n",
              "      <td>85.37</td>\n",
              "      <td>72.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>63.64</td>\n",
              "      <td>85.37</td>\n",
              "      <td>72.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>64.15</td>\n",
              "      <td>82.93</td>\n",
              "      <td>72.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "70       100.0    7.0        70.0     71.59     65.38      82.93  73.12\n",
              "51       100.0    5.0        50.0     71.59     65.38      82.93  73.12\n",
              "42       100.0    5.0        40.0     70.45     63.64      85.37  72.92\n",
              "21        50.0    5.0        20.0     70.45     63.64      85.37  72.92\n",
              "61       100.0    7.0        60.0     70.45     64.15      82.93  72.34\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "81        10.0    5.0        90.0     53.41      0.00       0.00   0.00\n",
              "63        10.0    5.0        70.0     53.41      0.00       0.00   0.00\n",
              "73        10.0    7.0        80.0     53.41      0.00       0.00   0.00\n",
              "72        10.0    5.0        80.0     53.41      0.00       0.00   0.00\n",
              "36        10.0    5.0        40.0     53.41      0.00       0.00   0.00\n",
              "\n",
              "[99 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VW7l6w9-Yu0"
      },
      "source": [
        "Melhores parâmetros:\n",
        "Repetições = 100\n",
        "Folds = 5\n",
        "Tolerância = 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnddgwlt-h8W",
        "outputId": "18d7a357-0ce5-480b-c9cf-9bb5d2275876"
      },
      "source": [
        "y_previsao_forest, scores_forest = modelo_repetido(modelo_forest,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 100,n_folds = 5, tol= 50, verbosity = 1,\n",
        "                             tol_overfit = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 500 iterações\n",
            "Serão utilizados 370 modelos no processo\n",
            "Um total de 130 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.62      0.70        47\n",
            "           1       0.65      0.83      0.73        41\n",
            "\n",
            "    accuracy                           0.72        88\n",
            "   macro avg       0.73      0.72      0.71        88\n",
            "weighted avg       0.73      0.72      0.71        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92       142\n",
            "           1       0.86      1.00      0.92       121\n",
            "\n",
            "    accuracy                           0.92       263\n",
            "   macro avg       0.93      0.93      0.92       263\n",
            "weighted avg       0.93      0.92      0.92       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fueF6vs41Pys"
      },
      "source": [
        "Aqui novamente notamos o jogo do 'toma lá dá cá': conseguimos melhorar a acurácia, mas às custas de piorar nossa precisão para negativos. Ainda assim, parece ser o melhor modelo até o momento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poe9IM7WESR4"
      },
      "source": [
        "## KNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD9aJTrGExPC",
        "outputId": "c7485327-15fe-4c3e-ef6c-7a3e506830c3"
      },
      "source": [
        "modelo_kn = KNeighborsClassifier(algorithm = 'auto',\n",
        "                                 metric = 'manhattan',\n",
        "                                 n_neighbors = 14,\n",
        "                                 weights = 'distance')\n",
        "\n",
        "modelo_kn.fit(X_treino, y_treino)\n",
        "y_pred_kn = modelo_kn.predict(X_teste)\n",
        "acur = modelo_kn.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia = 59.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd5pZPLmIrGk",
        "outputId": "a61f5bf9-72fc-4dc4-8250-3651d61e88ca"
      },
      "source": [
        "print(classification_report(y_treino,modelo_kn.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       142\n",
            "           1       1.00      1.00      1.00       121\n",
            "\n",
            "    accuracy                           1.00       263\n",
            "   macro avg       1.00      1.00      1.00       263\n",
            "weighted avg       1.00      1.00      1.00       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkTOgmJEIrG8",
        "outputId": "37e27091-2a6e-42f5-fb86-f7e33a3557ea"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_kn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.79      0.67        47\n",
            "           1       0.60      0.37      0.45        41\n",
            "\n",
            "    accuracy                           0.59        88\n",
            "   macro avg       0.59      0.58      0.56        88\n",
            "weighted avg       0.59      0.59      0.57        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJ9bogo1tQN"
      },
      "source": [
        "Esse modelo foi péssimo: sofreu um overfitting grande, que fez ter métricas muito ruins nos dados de teste. Nosso modelo baseline é melhor que isso. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJdiLUCcI0wl",
        "outputId": "022662c0-86b1-4c58-ec35-e3a6db11d0d4"
      },
      "source": [
        "y_previsao_kn, scores_kn = modelo_repetido(modelo_kn,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0,\n",
        "                             tol_overfit = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 50 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.74      0.70        47\n",
            "           1       0.66      0.56      0.61        41\n",
            "\n",
            "    accuracy                           0.66        88\n",
            "   macro avg       0.66      0.65      0.65        88\n",
            "weighted avg       0.66      0.66      0.66        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92       142\n",
            "           1       0.86      1.00      0.92       121\n",
            "\n",
            "    accuracy                           0.92       263\n",
            "   macro avg       0.93      0.93      0.92       263\n",
            "weighted avg       0.93      0.92      0.92       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot5nKzMo19dU"
      },
      "source": [
        "Utilizando o modelo_repetido(),conseguimos melhorar um pouco, porém a regressão logística ainda é melhor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcrAwWOvJoly",
        "outputId": "eca19838-1544-4d91-dfb6-018f90337ea8"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_kn[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[35 12]\n",
            " [18 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sPaJwAeJolz",
        "outputId": "1860883d-54ad-4bb2-fd55-a49799951a1e"
      },
      "source": [
        "print(confusion_matrix(y_treino,y_previsao_kn[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[122  20]\n",
            " [  0 121]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "VD4aR3TVJol0",
        "outputId": "acadc3b5-1135-4973-af98-7551ed6a3b62"
      },
      "source": [
        "import itertools\n",
        "df_scores_kn = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10,50,100]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  y_previsao_kn, scores_kn = modelo_repetido(modelo_kn,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0,\n",
        "                             tol_overfit = 2)\n",
        "  if scores_forest == False:\n",
        "    pass\n",
        "  else:\n",
        "    ac = scores_kn['Acuracia'][0]\n",
        "    pr = scores_kn['Precisão'][0]\n",
        "    rv = scores_kn['Revocação'][0]\n",
        "    f1 = scores_kn['F1'][0]\n",
        "    nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "    df_scores_kn.loc[ind] = nova_coluna\n",
        "    ind = ind+1\n",
        "  \n",
        "df_scores_kn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.10</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.64</td>\n",
              "      <td>63.64</td>\n",
              "      <td>51.22</td>\n",
              "      <td>56.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.50</td>\n",
              "      <td>62.50</td>\n",
              "      <td>48.78</td>\n",
              "      <td>54.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.10</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.10</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>59.09</td>\n",
              "      <td>58.62</td>\n",
              "      <td>41.46</td>\n",
              "      <td>48.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>59.09</td>\n",
              "      <td>58.62</td>\n",
              "      <td>41.46</td>\n",
              "      <td>48.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>59.09</td>\n",
              "      <td>58.62</td>\n",
              "      <td>41.46</td>\n",
              "      <td>48.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>60.23</td>\n",
              "      <td>60.00</td>\n",
              "      <td>43.90</td>\n",
              "      <td>50.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>60.23</td>\n",
              "      <td>60.00</td>\n",
              "      <td>43.90</td>\n",
              "      <td>50.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0         10.0    5.0         0.0     65.91     65.71      56.10  60.53\n",
              "1         10.0    7.0         0.0     63.64     63.64      51.22  56.76\n",
              "2         10.0   10.0         0.0     62.50     62.50      48.78  54.79\n",
              "3         50.0    5.0         0.0     65.91     65.71      56.10  60.53\n",
              "4         50.0    7.0         0.0     65.91     65.71      56.10  60.53\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "94        50.0    7.0       100.0     59.09     58.62      41.46  48.57\n",
              "95        50.0   10.0       100.0     59.09     58.62      41.46  48.57\n",
              "96       100.0    5.0       100.0     59.09     58.62      41.46  48.57\n",
              "97       100.0    7.0       100.0     60.23     60.00      43.90  50.70\n",
              "98       100.0   10.0       100.0     60.23     60.00      43.90  50.70\n",
              "\n",
              "[99 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Ue2dDUjIRNH0",
        "outputId": "dc1d2552-ba25-405b-e544-cab8fa01fa77"
      },
      "source": [
        "df_scores_kn.sort_values(by = 'Acurácia', ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.1</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.1</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.1</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.1</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.91</td>\n",
              "      <td>65.71</td>\n",
              "      <td>56.1</td>\n",
              "      <td>60.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0         10.0    5.0         0.0     65.91     65.71       56.1  60.53\n",
              "15       100.0    5.0        10.0     65.91     65.71       56.1  60.53\n",
              "3         50.0    5.0         0.0     65.91     65.71       56.1  60.53\n",
              "4         50.0    7.0         0.0     65.91     65.71       56.1  60.53\n",
              "5         50.0   10.0         0.0     65.91     65.71       56.1  60.53\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "91        10.0    7.0       100.0     53.41      0.00        0.0   0.00\n",
              "63        10.0    5.0        70.0     53.41      0.00        0.0   0.00\n",
              "73        10.0    7.0        80.0     53.41      0.00        0.0   0.00\n",
              "81        10.0    5.0        90.0     53.41      0.00        0.0   0.00\n",
              "64        10.0    7.0        70.0     53.41      0.00        0.0   0.00\n",
              "\n",
              "[99 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgWoau9pRT8R"
      },
      "source": [
        "Melhores parâmetros:\n",
        "\n",
        "Repetições = 10\n",
        "Folds = 5\n",
        "Tolerância = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud-Tr6mr2RUr",
        "outputId": "ebd79775-557a-4fc9-8c95-07b3304a2a9d"
      },
      "source": [
        "y_previsao_kn, scores_kn = modelo_repetido(modelo_kn,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0, verbosity = 1,\n",
        "                             tol_overfit = 2)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 50 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.74      0.70        47\n",
            "           1       0.66      0.56      0.61        41\n",
            "\n",
            "    accuracy                           0.66        88\n",
            "   macro avg       0.66      0.65      0.65        88\n",
            "weighted avg       0.66      0.66      0.66        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92       142\n",
            "           1       0.86      1.00      0.92       121\n",
            "\n",
            "    accuracy                           0.92       263\n",
            "   macro avg       0.93      0.93      0.92       263\n",
            "weighted avg       0.93      0.92      0.92       263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUU8OBG1n1KP"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_3WHRemoG2o",
        "outputId": "5e72d8ce-ef82-4797-b535-53f28237fa6a"
      },
      "source": [
        "modelo_xgb = XGBClassifier(random_state=527435,\n",
        "                           booster = 'dart',\n",
        "                           eta = 0.1,\n",
        "                           max_depth = 5,\n",
        "                           sampling_method = 'uniform',\n",
        "                           subsample = 0.7)\n",
        "\n",
        "modelo_xgb.fit(X_treino, y_treino)\n",
        "y_pred_xgb = modelo_xgb.predict(X_teste)\n",
        "acur = modelo_xgb.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia = 75.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqfVKbKBpNeO",
        "outputId": "19a6caad-0111-4b44-fae4-28cd72e0aad9"
      },
      "source": [
        "print(classification_report(y_treino,modelo_xgb.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       142\n",
            "           1       1.00      1.00      1.00       121\n",
            "\n",
            "    accuracy                           1.00       263\n",
            "   macro avg       1.00      1.00      1.00       263\n",
            "weighted avg       1.00      1.00      1.00       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLLlFZfLpNeV",
        "outputId": "1b7fdd41-5e34-44a0-a9fd-fa3a3bdab92f"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_xgb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.74      0.76        47\n",
            "           1       0.72      0.76      0.74        41\n",
            "\n",
            "    accuracy                           0.75        88\n",
            "   macro avg       0.75      0.75      0.75        88\n",
            "weighted avg       0.75      0.75      0.75        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlbYSr452srU"
      },
      "source": [
        "Tivemos novamente um overfitting total, mas as métricas dos dados de testes não estão tão ruins, e até agora se mostraram as mais equilibradas. \n",
        "Vejamos se melhoram com o modelo_repetido():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwWaFe4DpNeW",
        "outputId": "4a9737cb-81fe-4528-d083-b9a35dd2d9de"
      },
      "source": [
        "y_previsao_xgb, scores_xgb = modelo_repetido(modelo_xgb,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0,\n",
        "                             tol_overfit = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 50 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.40      0.57        47\n",
            "           1       0.59      0.98      0.73        41\n",
            "\n",
            "    accuracy                           0.67        88\n",
            "   macro avg       0.77      0.69      0.65        88\n",
            "weighted avg       0.78      0.67      0.64        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.59      0.74       142\n",
            "           1       0.68      1.00      0.81       121\n",
            "\n",
            "    accuracy                           0.78       263\n",
            "   macro avg       0.84      0.80      0.78       263\n",
            "weighted avg       0.85      0.78      0.77       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m5gukZP2_S4"
      },
      "source": [
        "Novamente, ganha-se em algo e perde-se em outro ponto. Nossa precisão para negativos aumentou consideravelmente, tornando um bom modelo para atacar o problema 2. Na média, entretanto, as métricas pioraram, puxadas pela queda considerável nas outras duas métricas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU2etgBWroP5",
        "outputId": "2953bb5a-e040-46ee-a5ed-295bbb18c11c"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_xgb[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19 28]\n",
            " [ 1 40]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHKY91EroQE",
        "outputId": "27fb341b-6aae-4f15-8187-6e583e874085"
      },
      "source": [
        "print(confusion_matrix(y_treino,y_previsao_xgb[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 84  58]\n",
            " [  0 121]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjTFAgJroQE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e438d1cd-244e-4e29-ebbe-0472b572b445"
      },
      "source": [
        "import itertools\n",
        "df_scores_xgb = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10,50,100]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  y_previsao_xgb, xgb_scores = modelo_repetido(modelo_xgb,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0,\n",
        "                             tol_overfit = 1)\n",
        "  if xgb_scores == False:\n",
        "    pass\n",
        "  else:\n",
        "    ac = xgb_scores['Acuracia'][0]\n",
        "    pr = xgb_scores['Precisão'][0]\n",
        "    rv = xgb_scores['Revocação'][0]\n",
        "    f1 = xgb_scores['F1'][0]\n",
        "    nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "    df_scores_xgb.loc[ind] = nova_coluna\n",
        "    ind = ind+1\n",
        "  \n",
        "df_scores_xgb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>67.44</td>\n",
              "      <td>70.73</td>\n",
              "      <td>69.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.73</td>\n",
              "      <td>69.77</td>\n",
              "      <td>73.17</td>\n",
              "      <td>71.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>69.57</td>\n",
              "      <td>78.05</td>\n",
              "      <td>73.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.05</td>\n",
              "      <td>60.00</td>\n",
              "      <td>87.80</td>\n",
              "      <td>71.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>66.67</td>\n",
              "      <td>87.80</td>\n",
              "      <td>75.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0         10.0    5.0         0.0     70.45     67.44      70.73  69.05\n",
              "1         10.0    7.0         0.0     72.73     69.77      73.17  71.43\n",
              "2         50.0    5.0         0.0     73.86     69.57      78.05  73.56\n",
              "3         50.0    7.0         0.0     67.05     60.00      87.80  71.29\n",
              "4         50.0   10.0         0.0     73.86     66.67      87.80  75.79\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "83        50.0    7.0       100.0     53.41      0.00       0.00   0.00\n",
              "84        50.0   10.0       100.0     53.41      0.00       0.00   0.00\n",
              "85       100.0    5.0       100.0     53.41      0.00       0.00   0.00\n",
              "86       100.0    7.0       100.0     53.41      0.00       0.00   0.00\n",
              "87       100.0   10.0       100.0     53.41      0.00       0.00   0.00\n",
              "\n",
              "[88 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSIEwh1UHNTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4bcedf1d-0ff7-4bd6-b3ae-9b41cb4a6186"
      },
      "source": [
        "df_scores_xgb.sort_values(by = 'Acurácia', ascending= False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>71.43</td>\n",
              "      <td>73.17</td>\n",
              "      <td>72.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>69.57</td>\n",
              "      <td>78.05</td>\n",
              "      <td>73.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>66.67</td>\n",
              "      <td>87.80</td>\n",
              "      <td>75.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.73</td>\n",
              "      <td>67.35</td>\n",
              "      <td>80.49</td>\n",
              "      <td>73.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.73</td>\n",
              "      <td>69.77</td>\n",
              "      <td>73.17</td>\n",
              "      <td>71.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>53.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "14       100.0    7.0        10.0     73.86     71.43      73.17  72.29\n",
              "2         50.0    5.0         0.0     73.86     69.57      78.05  73.56\n",
              "4         50.0   10.0         0.0     73.86     66.67      87.80  75.79\n",
              "5        100.0    5.0         0.0     72.73     67.35      80.49  73.33\n",
              "1         10.0    7.0         0.0     72.73     69.77      73.17  71.43\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "33        10.0    7.0        40.0     53.41      0.00       0.00   0.00\n",
              "32        10.0    5.0        40.0     53.41      0.00       0.00   0.00\n",
              "31       100.0   10.0        30.0     53.41      0.00       0.00   0.00\n",
              "30       100.0    7.0        30.0     53.41      0.00       0.00   0.00\n",
              "87       100.0   10.0       100.0     53.41      0.00       0.00   0.00\n",
              "\n",
              "[88 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq_-Ir07iF1S"
      },
      "source": [
        "Melhores parâmetros:\n",
        "\n",
        "Repetições = 50\n",
        "Folds = 10\n",
        "Tolerância = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb_K2GLK3b74",
        "outputId": "0135c29c-d6f8-4f23-c91f-ce966955a712"
      },
      "source": [
        "y_previsao_xgb, scores_xgb = modelo_repetido(modelo_xgb,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 50,n_folds = 10, tol= 0, verbosity = 1,\n",
        "                             tol_overfit = 2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foram realizadas 500 iterações\n",
            "Serão utilizados 500 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.38      0.55        47\n",
            "           1       0.58      0.98      0.73        41\n",
            "\n",
            "    accuracy                           0.66        88\n",
            "   macro avg       0.76      0.68      0.64        88\n",
            "weighted avg       0.78      0.66      0.63        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.56      0.71       142\n",
            "           1       0.66      1.00      0.79       121\n",
            "\n",
            "    accuracy                           0.76       263\n",
            "   macro avg       0.83      0.78      0.75       263\n",
            "weighted avg       0.84      0.76      0.75       263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bux7tD22-gdK"
      },
      "source": [
        "## GradientBoostting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdAN9-XD-lan",
        "outputId": "2122d363-26bc-4f01-b9d2-492dfb030595"
      },
      "source": [
        "modelo_grad = GradientBoostingClassifier(random_state=527435,\n",
        "                                         criterion = 'mse',\n",
        "                                         learning_rate = 0.01,\n",
        "                                         loss = 'exponential',\n",
        "                                         max_depth = 5,\n",
        "                                         max_features = 'log2')\n",
        "\n",
        "modelo_grad.fit(X_treino, y_treino)\n",
        "y_pred_grad = modelo_grad.predict(X_teste)\n",
        "acur = modelo_grad.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia = 71.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqotWSK8_-Qq",
        "outputId": "d8236081-8720-482c-b668-91ccc77f9093"
      },
      "source": [
        "print(classification_report(y_treino,modelo_grad.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96       142\n",
            "           1       1.00      0.89      0.94       121\n",
            "\n",
            "    accuracy                           0.95       263\n",
            "   macro avg       0.96      0.95      0.95       263\n",
            "weighted avg       0.95      0.95      0.95       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50HewqHv_-Qw",
        "outputId": "628c8a51-1d4f-4da6-f3b3-7f9d0c67c1b5"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_grad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.74      0.74        47\n",
            "           1       0.70      0.68      0.69        41\n",
            "\n",
            "    accuracy                           0.72        88\n",
            "   macro avg       0.71      0.71      0.71        88\n",
            "weighted avg       0.72      0.72      0.72        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Vb2Zaw31Cw"
      },
      "source": [
        "Novamente, um modelo com métricas equilibradas, semelhante ao XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YroUFse4_-Qw",
        "outputId": "2c271826-52e1-400c-eae9-f393d5393fbc"
      },
      "source": [
        "y_previsao_grad, scores_grad = modelo_repetido(modelo_grad,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0,\n",
        "                             tol_overfit = 0.97)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 44 modelos no processo\n",
            "Um total de 6 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.51      0.65        47\n",
            "           1       0.62      0.93      0.75        41\n",
            "\n",
            "    accuracy                           0.70        88\n",
            "   macro avg       0.76      0.72      0.70        88\n",
            "weighted avg       0.76      0.70      0.69        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.67      0.79       142\n",
            "           1       0.71      0.97      0.82       121\n",
            "\n",
            "    accuracy                           0.81       263\n",
            "   macro avg       0.84      0.82      0.80       263\n",
            "weighted avg       0.85      0.81      0.80       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFuB8-wu4CdL"
      },
      "source": [
        "Como no caso de XGBoost, melhoramos a precisão para negativos, mas a média piorou. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqaEpFQJAetB",
        "outputId": "c80b0e69-6de9-4867-f40a-3f03cf43f4c3"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_grad[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[24 23]\n",
            " [ 3 38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk4QrNG5AetC",
        "outputId": "64a23baa-cffd-4f1c-b411-9c9c2ff7c8f1"
      },
      "source": [
        "print(confusion_matrix(y_treino,y_previsao_grad[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 95  47]\n",
            " [  4 117]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bHLSSi25AetD",
        "outputId": "3de85075-393a-400c-acb4-05db3c980782"
      },
      "source": [
        "import itertools\n",
        "df_scores_grad = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10,50,100]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  y_previsao_grad, grad_scores = modelo_repetido(modelo_grad,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0,\n",
        "                             tol_overfit = 0.97)\n",
        "  if grad_scores == False:\n",
        "    pass\n",
        "  else:\n",
        "    ac = grad_scores['Acuracia'][0]\n",
        "    pr = grad_scores['Precisão'][0]\n",
        "    rv = grad_scores['Revocação'][0]\n",
        "    f1 = grad_scores['F1'][0]\n",
        "    nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "    df_scores_grad.loc[ind] = nova_coluna\n",
        "    ind = ind+1\n",
        "  \n",
        "df_scores_grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Repetições</th>\n",
              "      <th>Folds</th>\n",
              "      <th>Tolerância</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Revocação</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>62.30</td>\n",
              "      <td>92.68</td>\n",
              "      <td>74.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.32</td>\n",
              "      <td>62.07</td>\n",
              "      <td>87.80</td>\n",
              "      <td>72.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.86</td>\n",
              "      <td>65.52</td>\n",
              "      <td>92.68</td>\n",
              "      <td>76.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>61.54</td>\n",
              "      <td>97.56</td>\n",
              "      <td>75.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.59</td>\n",
              "      <td>62.90</td>\n",
              "      <td>95.12</td>\n",
              "      <td>75.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>50.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>71.59</td>\n",
              "      <td>69.05</td>\n",
              "      <td>70.73</td>\n",
              "      <td>69.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>71.59</td>\n",
              "      <td>68.18</td>\n",
              "      <td>73.17</td>\n",
              "      <td>70.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>65.31</td>\n",
              "      <td>78.05</td>\n",
              "      <td>71.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>100.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>64.71</td>\n",
              "      <td>80.49</td>\n",
              "      <td>71.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.45</td>\n",
              "      <td>64.71</td>\n",
              "      <td>80.49</td>\n",
              "      <td>71.74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Repetições  Folds  Tolerância  Acurácia  Precisão  Revocação     F1\n",
              "0         10.0    5.0         0.0     70.45     62.30      92.68  74.51\n",
              "1         10.0    7.0         0.0     69.32     62.07      87.80  72.73\n",
              "2         10.0   10.0         0.0     73.86     65.52      92.68  76.77\n",
              "3         50.0    5.0         0.0     70.45     61.54      97.56  75.47\n",
              "4         50.0    7.0         0.0     71.59     62.90      95.12  75.73\n",
              "..         ...    ...         ...       ...       ...        ...    ...\n",
              "94        50.0    7.0       100.0     71.59     69.05      70.73  69.88\n",
              "95        50.0   10.0       100.0     71.59     68.18      73.17  70.59\n",
              "96       100.0    5.0       100.0     70.45     65.31      78.05  71.11\n",
              "97       100.0    7.0       100.0     70.45     64.71      80.49  71.74\n",
              "98       100.0   10.0       100.0     70.45     64.71      80.49  71.74\n",
              "\n",
              "[99 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rPRmRr3bEjL"
      },
      "source": [
        "Melhores parâmetros: \n",
        "\n",
        "Repetições = 100\n",
        "Folds = 5\n",
        "Tolerância = 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFX0Yw2S4YAW",
        "outputId": "2d94c692-8cef-4b07-939e-5ded1e9e8d29"
      },
      "source": [
        "y_previsao_grad, scores_grad = modelo_repetido(modelo_grad,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 100,n_folds = 5, tol= 10, verbosity = 1,\n",
        "                             tol_overfit = 1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foram realizadas 500 iterações\n",
            "Serão utilizados 500 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.57      0.71        47\n",
            "           1       0.66      0.95      0.78        41\n",
            "\n",
            "    accuracy                           0.75        88\n",
            "   macro avg       0.80      0.76      0.75        88\n",
            "weighted avg       0.81      0.75      0.74        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.70      0.81       142\n",
            "           1       0.74      0.97      0.84       121\n",
            "\n",
            "    accuracy                           0.83       263\n",
            "   macro avg       0.85      0.84      0.82       263\n",
            "weighted avg       0.86      0.83      0.82       263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXMOxez1jsLX"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYMMhlSskuCt",
        "outputId": "499b5b4f-6457-4556-a6e6-36d61be03de7"
      },
      "source": [
        "modelo_mlp = MLPClassifier(random_state=527435,\n",
        "                                         activation = 'relu',\n",
        "                                         learning_rate = 'constant',\n",
        "                                         max_iter = 100,\n",
        "                                         solver = 'adam',\n",
        "                                         tol = 0.0001)\n",
        "\n",
        "modelo_mlp.fit(X_treino, y_treino)\n",
        "y_pred_mlp = modelo_mlp.predict(X_teste)\n",
        "acur = modelo_mlp.score(X_teste, y_teste)\n",
        "print(f\"Acurácia = {round(100*acur, 2)}%\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia = 67.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfXP7WQElNYH",
        "outputId": "17f8d341-8cc8-45b6-e234-551b12417f6a"
      },
      "source": [
        "print(classification_report(y_treino,modelo_mlp.predict(X_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       142\n",
            "           1       0.84      0.76      0.80       121\n",
            "\n",
            "    accuracy                           0.83       263\n",
            "   macro avg       0.83      0.82      0.82       263\n",
            "weighted avg       0.83      0.83      0.82       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVt5R2Y7lNYX",
        "outputId": "5e19fdbc-4c44-4b1e-e04f-3e586e9c1b2b"
      },
      "source": [
        "print(classification_report(y_teste,y_pred_mlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.68      0.69        47\n",
            "           1       0.64      0.66      0.65        41\n",
            "\n",
            "    accuracy                           0.67        88\n",
            "   macro avg       0.67      0.67      0.67        88\n",
            "weighted avg       0.67      0.67      0.67        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFsMscQulNYX",
        "outputId": "a69f09d0-38c9-4351-a5d7-6f7279cea5ba"
      },
      "source": [
        "y_previsao_mlp, scores_mlp = modelo_repetido(modelo_mlp,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 5, tol= 0,\n",
        "                             tol_overfit = 0.95)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram realizadas 50 iterações\n",
            "Serão utilizados 50 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.57      0.64        47\n",
            "           1       0.61      0.76      0.67        41\n",
            "\n",
            "    accuracy                           0.66        88\n",
            "   macro avg       0.67      0.67      0.66        88\n",
            "weighted avg       0.67      0.66      0.66        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.63      0.72       142\n",
            "           1       0.67      0.86      0.75       121\n",
            "\n",
            "    accuracy                           0.74       263\n",
            "   macro avg       0.75      0.75      0.74       263\n",
            "weighted avg       0.76      0.74      0.74       263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yS6c-jDl7Dl",
        "outputId": "9cf387a2-4eca-4b6a-cb9a-1591baefd5da"
      },
      "source": [
        "print(confusion_matrix(y_teste,y_previsao_mlp[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27 20]\n",
            " [10 31]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieDTmqcxl7Dn",
        "outputId": "69a0a2ca-f183-48fc-a886-fb44ecb2b4a5"
      },
      "source": [
        "print(confusion_matrix(y_treino,y_previsao_mlp[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 90  52]\n",
            " [ 17 104]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGOkJvFnKwM"
      },
      "source": [
        "import itertools\n",
        "df_scores_mlp = pd.DataFrame(columns=['Repetições', 'Folds', 'Tolerância', 'Acurácia', 'Precisão', 'Revocação', 'F1'])\n",
        "T = [10*x for x in range(0,11)]\n",
        "R = [10,50,100]\n",
        "F = [5,7,10]\n",
        "ind = 0\n",
        "for t,r,f in itertools.product(T,R,F):\n",
        "  mlp_previsao, mlp_scores = modelo_repetido(melhor_modelo_mlp,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = r,n_folds = f, tol= t, verbosity = 0,\n",
        "                             tol_overfit = 1)\n",
        "  if mlp_scores == False:\n",
        "    pass\n",
        "  else:\n",
        "    ac = mlp_scores['Acuracia'][0]\n",
        "    pr = mlp_scores['Precisão'][0]\n",
        "    rv = mlp_scores['Revocação'][0]\n",
        "    f1 = mlp_scores['F1'][0]\n",
        "    nova_coluna = [r,f,t,ac,pr,rv,f1]\n",
        "    df_scores_mlp.loc[ind] = nova_coluna\n",
        "    ind = ind+1\n",
        "  \n",
        "df_scores_mlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlI4DHHqnN47"
      },
      "source": [
        "df_scores_mlp.sort_values(by = 'Acurácia', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXzRidLTp8Kj"
      },
      "source": [
        "Melhores parâmetros\n",
        "\n",
        "Repetições = 10\n",
        "Folds = 10\n",
        "Tolerância = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6AgDEMu5kL4",
        "outputId": "2e2e71b0-3095-4a91-e7a9-9ebf4bab6301"
      },
      "source": [
        "y_previsao_mlp, scores_mlp = modelo_repetido(modelo_mlp,\n",
        "                            X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             n_rep = 10,n_folds = 10, tol= 100, verbosity = 1,\n",
        "                             tol_overfit = 1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foram realizadas 100 iterações\n",
            "Serão utilizados 100 modelos no processo\n",
            "Um total de 0 modelos tiveram overfitting com os dados de treino\n",
            "\n",
            " || Relatório de classificação (dados de teste) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.77      0.73        47\n",
            "           1       0.70      0.63      0.67        41\n",
            "\n",
            "    accuracy                           0.70        88\n",
            "   macro avg       0.70      0.70      0.70        88\n",
            "weighted avg       0.70      0.70      0.70        88\n",
            "\n",
            "\n",
            " || Relatório de classificação (dados de treino) || \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      1.00      0.70       142\n",
            "           1       0.00      0.00      0.00       121\n",
            "\n",
            "    accuracy                           0.54       263\n",
            "   macro avg       0.27      0.50      0.35       263\n",
            "weighted avg       0.29      0.54      0.38       263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PriL8Q0ndHt"
      },
      "source": [
        "# Modelo Final "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzATal253Cn"
      },
      "source": [
        "Finalmente, iremos construir nosso modelo com base em todos os modelos intermediários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2z77Qh8ne_N"
      },
      "source": [
        "def varios_modelos_repetidos(X_train, y_train, X_test, y_test, tol = 0, prob = False):\n",
        "  modelo_reg_log = LogisticRegression(solver = 'lbfgs',\n",
        "                                    C = 1.0,\n",
        "                                    class_weight = None,\n",
        "                                    fit_intercept = True,\n",
        "                                    max_iter = 50,\n",
        "                                    tol = 0.1,\n",
        "                                    random_state = 527435)\n",
        "  modelo_forest = RandomForestClassifier(criterion= 'gini', max_depth= 15, max_features= 'log2',\n",
        "                                class_weight='balanced_subsample', max_samples = 0.6,\n",
        "                                random_state= 527435)\n",
        "  modelo_kn = KNeighborsClassifier(algorithm = 'auto',\n",
        "                                 metric = 'manhattan',\n",
        "                                 n_neighbors = 14,\n",
        "                                 weights = 'distance')\n",
        "  modelo_xgb = XGBClassifier(random_state=527435,\n",
        "                           booster = 'dart',\n",
        "                           eta = 0.1,\n",
        "                           max_depth = 5,\n",
        "                           sampling_method = 'uniform',\n",
        "                           subsample = 0.7)\n",
        "  modelo_grad = GradientBoostingClassifier(random_state=527435,\n",
        "                                         criterion = 'mse',\n",
        "                                         learning_rate = 0.01,\n",
        "                                         loss = 'exponential',\n",
        "                                         max_depth = 5,\n",
        "                                         max_features = 'log2')\n",
        "  modelo_mlp = MLPClassifier(random_state=527435,\n",
        "                                         activation = 'relu',\n",
        "                                         learning_rate = 'constant',\n",
        "                                         max_iter = 100,\n",
        "                                         solver = 'adam',\n",
        "                                         tol = 0.0001)\n",
        "\n",
        "\n",
        "  lista_prev_parc = []\n",
        "\n",
        "  prev_log_reg, log_reg_sc = modelo_repetido(modelo_reg_log,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 20, n_folds = 5, tol = 90, verbosity = 0, prob = prob)\n",
        "  if prev_log_reg == False:\n",
        "    print('Modelo Logistic Regression não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_log_reg[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_log_reg)\n",
        "\n",
        "  prev_forest, forest_sc = modelo_repetido(modelo_forest,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 100, n_folds = 5, tol = 50,\n",
        "                                           tol_overfit = 1, verbosity = 0, prob = prob)\n",
        "  if prev_forest == False:\n",
        "    print('Modelo Random Forest não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_forest[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_forest)\n",
        "  \n",
        "  prev_kn, kn_sc = modelo_repetido(modelo_kn,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 10, n_folds = 5, tol = 0,\n",
        "                                   tol_overfit = 2, verbosity = 0, prob = prob)\n",
        "  if prev_kn == False:\n",
        "    print('Modelo KNeighborns não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_kn[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_kn)\n",
        "  \n",
        "  prev_xgb, xgb_sc = modelo_repetido(modelo_xgb,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 50, n_folds = 10, tol = 0,\n",
        "                                     tol_overfit = 1,verbosity = 0, prob = prob)\n",
        "  if prev_xgb == False:\n",
        "    print('Modelo XGB não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_xgb[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_xgb)\n",
        "  \n",
        "  prev_grad, grad_sc = modelo_repetido(modelo_grad,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 100, n_folds = 5, tol = 10,\n",
        "                                            tol_overfit = 1, verbosity = 0, prob = prob)\n",
        "  if prev_grad == False:\n",
        "    print('Modelo Gradient Boosting não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_grad[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_grad)\n",
        "  \n",
        "  prev_mlp, mlp_sc = modelo_repetido(modelo_kn,\n",
        "                                             X_train = X_train, y_train = y_train,\n",
        "                                             X_test = X_test, y_test = y_test,\n",
        "                                             n_rep = 10, n_folds = 10, tol = 100,\n",
        "                                     tol_overfit = 2, verbosity = 0, prob = prob)\n",
        "  if prev_mlp == False:\n",
        "    print('Modelo MLP não retornou resultado')\n",
        "  else:\n",
        "    if prob == False:\n",
        "      lista_prev_parc.append(prev_mlp[0])\n",
        "    else:\n",
        "      lista_prev_parc.append(prev_mlp)    \n",
        "\n",
        "\n",
        "  prev_final = compara_lista_series_binarias(lista_prev_parc, tolerancia= tol)\n",
        "\n",
        "  print(classification_report(y_test, prev_final))\n",
        "\n",
        "  return prev_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVAR2pdPwpNn",
        "outputId": "66c0040a-7787-4555-a0dc-1d378539f663"
      },
      "source": [
        "for tl in [0,1,2,3,4,5,6]:\n",
        "  y_prev = varios_modelos_repetidos(X_train =  X_treino, y_train =  y_treino,\n",
        "                             X_test =  X_teste, y_test =  y_teste,\n",
        "                             tol = tl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.51      0.66        47\n",
            "           1       0.63      0.95      0.76        41\n",
            "\n",
            "    accuracy                           0.72        88\n",
            "   macro avg       0.78      0.73      0.71        88\n",
            "weighted avg       0.79      0.72      0.70        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.53      0.67        47\n",
            "           1       0.63      0.93      0.75        41\n",
            "\n",
            "    accuracy                           0.72        88\n",
            "   macro avg       0.76      0.73      0.71        88\n",
            "weighted avg       0.77      0.72      0.71        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.66      0.75        47\n",
            "           1       0.69      0.88      0.77        41\n",
            "\n",
            "    accuracy                           0.76        88\n",
            "   macro avg       0.78      0.77      0.76        88\n",
            "weighted avg       0.78      0.76      0.76        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.72      0.69        47\n",
            "           1       0.64      0.56      0.60        41\n",
            "\n",
            "    accuracy                           0.65        88\n",
            "   macro avg       0.65      0.64      0.64        88\n",
            "weighted avg       0.65      0.65      0.65        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.85      0.75        47\n",
            "           1       0.75      0.51      0.61        41\n",
            "\n",
            "    accuracy                           0.69        88\n",
            "   macro avg       0.71      0.68      0.68        88\n",
            "weighted avg       0.71      0.69      0.68        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.94      0.72        47\n",
            "           1       0.75      0.22      0.34        41\n",
            "\n",
            "    accuracy                           0.60        88\n",
            "   macro avg       0.66      0.58      0.53        88\n",
            "weighted avg       0.66      0.60      0.54        88\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      1.00      0.70        47\n",
            "           1       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.53        88\n",
            "   macro avg       0.27      0.50      0.35        88\n",
            "weighted avg       0.29      0.53      0.37        88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G5nN4JjkyBJ"
      },
      "source": [
        "Percebemos que tolerância = 2 é a melhor parâmetro.\n",
        "Obtivemos uma acurácia de 77%, a maior até o momento. Olhando para as outras métricas, percebemos os f1 scores também acima de 75%, o que também foram os melhores até o momento. \n",
        "\n",
        "Como na maioria dos modelos que vimos, esse também parece ser mais adequado para lidar com o problema 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DgvrnP_7azI"
      },
      "source": [
        "# Resumo e considerações finais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yDtvAhj7dDL"
      },
      "source": [
        "Criamos um modelo baseline 'na mão' que nos dava uma acurácia de 64%. Com excessão do kneighbors, todos os outros modelos tiveram melhor desempenho que o modelo de base. \n",
        "\n",
        "Para certas escolhas de parâmetros, os modelos RandomForest, XGBoost e GradientBoosting são excelentes para lidar com o problema 2: prever que um paciente não vai para a UTI. \n",
        "\n",
        "No entanto, nenhum dos modelos foi realmente bom para atacar o problema 1: prever que um paciente vai para a UTI. \n",
        "\n",
        "De modo que em resumo, podemos dizer que temos um modelo que responde o problema 2, mas não temos um modelo que responde o problema 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdXRjQa3-R-O"
      },
      "source": [
        "Nosso modelo final é o que tem a melhor compensação entre equilíbrio entre as métricas e métricas boas. No entanto, não seria realmente bom para atacar um dos dois problemas, apesar de se sair razoavelmente bem com o problema 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKBOKcLb-2Mb"
      },
      "source": [
        "Alguns pontos que podem ser melhorados nesse projeto, como um todo:\n",
        "- Explorar técnicas melhores de tunnig de hiperparâmetros. GridSearch é muito custoso computacionalmente falando.\n",
        "- Implementar as funções criadas aqui nesse notebook como métodos dentro de uma classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qU2nQdr_fXg"
      },
      "source": [
        "Ideias que podem ser exploradas na tentativa de melhorar os modelos:\n",
        "- Tentar outras maneiras de fazer o 'emsamble' de modelos, dando pesos diferentes para cada um. \n",
        "- Incluir de alguma forma o modelo de base no emsamble.\n",
        "- Tentar modelos que não forma utilizados aqui, como por exemplo SVC e Redes Neurais. \n",
        "\n",
        "Além disso, podemos melhorar outras perspecitivas como por exemplo, fazer um 'feature engineering' diferente do que foi feito no notebook de preparação de dados e análise exploratória. \n",
        "\n",
        "Por fim, podemos mudar o problema e tentar utilizar isso a nosso favor. \n",
        "Experimentamos um modelo RandomForest com hiperâmetros padrões, sem feature engineering complexa nos dados que tenta prever se um paciente vai para a UTI na janela seguinte. Obtivemos valores em torno de 98% em todas as métricas. \n",
        "Obviamente, esse não é o problema inicial, e na prática pode ser inútil prever se um paciente vai precisar de UTI nas próximas 2 horas. É provável que os testes sanguíneos nem estejam prontos nesse intervalo de tempo.\n",
        "Porém, pode ser útil utilizar esse modelo para prever quais são os pacientes mais imediatos e então tentar utilizar outros modelos em outros pacientes. Explicando melhor, se esse modelo prever que um paciente não irá para a UTI na próxima janela, podemos jogar os dados desse paciente em outro modelo menos acurado que prevê se ele vai precisar de UTI em janelas de tempo mais distantes.\n",
        "Outra ideia é tentar utilizar apenas os sinais vitais (que podem ser medidas quase que em tempo real) como variáveis que serão jogadas nos modelos. \n",
        "Tudo isso são ideias para projetos futuros. "
      ]
    }
  ]
}